For this report, we have computed the analytical solution to the heat equation for a rod of length $1$. 
Furthermore, we used the finite difference method and a NN to find a numerical solution to the same problem. 
In this section, we describe our implementations of the two methods. 

\subsection{Finite difference method}

We implement the finite difference method as a stepping algorithm through time.
Using a discrete mesh with distance $\Delta x$ between nodes in the spac values for each time step is represented as a vector $$

\subsection{Neural Networks}

We have trained several NN models to predict the heat diffusion from a rod of length 1.
We utilized the PyTorch library \cite{Ansel_PyTorch_2_Faster_2024} for the neural network code. 
Adam was our optimizer of choice and has been used for all models. 
The weights were initialized using a Xavier Glorot normal distribution as described in Sec. \ref{sec:NN_init}. 
As the activation function for the output layer, $identity$ was used, i.e. no activation. 
Given the problem at hand, the size of the initial layer was $2$ and the output layer $1$. 
Furthermore, we specified a discretization of the $(x,t)$-grid, the batch size, and the number of epochs. 
The values for these hyperparameters are held constant across all models and can be found in Tab. \ref{tab:valuesfornn}. 
% \mia{early stopping if we include it}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Epochs & 1000  \\ \hline
        Batch size & 3000 \\\hline
        Learning rate & 0.001 \\ \hline
        Number of points along x-axis ($N_x$) & 100 \\ \hline
        Number of points along t-axis ($N_t$) & 100 \\ \hline
    \end{tabular}
    \caption{Values of hyperparameters that are common for all models}
    \label{tab:valuesfornn}
\end{table}

% \mia{initial layer and final layer size, train test split}
% \mia{how many iterations/seeds, choosing all seeds explicitly in order to be able to run parts of the code over again}

We wanted to find the best NN model and explore the properties of some different hyperparameters. 
First, we launched a three-dimensional grid search over the \textit{number of hidden layers}, the \textit{size of the hidden layers}, and the \textit{activation functions}. 
The size of the hidden layers dictates the width of the NN, while the number of (hidden) layers gives the depth. 
All possible combinations of the values given in Tab. \ref{tab:hyperparams} were used to train a NN model. 
The same activation function was used for all hidden layers, which all had the same size. 
We compared the final output of the model to the analytical solution using mean squared error (MSE). 
The analytical solution was never used during training and only utilized to evaluate the final models. 
We chose the best model in terms of MSE and kept track of the hyperparameters that produced it. 

To further explore the properties of the hyperparameters in Tab. \ref{tab:hyperparams}, we trained more models. 
For one of the three hyperparameters in Tab. \ref{tab:hyperparams} at a time, we kept the the best value (found in the grid search) for the other two, and trained ten models for each of the values for the hyperparameter. 
I.e. for \textit{activation functions} we trained 10 models for each of the four different alternatives while keeping the width and the depth of the network to the best values. 


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Number of hidden layers & 1, 2, 3  \\ \hline
        Size of hidden layers & 10, 25, 50, 100 \\\hline
        Activation functions & tanh, ReLU, leaky ReLU, sigmoid \\ \hline
    \end{tabular}
    \caption{Hyperparameters used in the grid search for the best neural network model.}
    \label{tab:hyperparams}
\end{table}
