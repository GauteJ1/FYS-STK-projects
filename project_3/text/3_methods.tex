For this report, we have computed the analytical solution to the heat equation for a rod of length $1$. 
Furthermore, we have used the finite difference method and a NN to numerically find the solution to the same problem. 
In this section, we describe our implementations of the two methods. 

\subsection{Finite difference method}



\subsection{Neural Networks}

\mia{rename neural network to NN as Julie has done in theory section}

We have trained several neural network models to predict the heat diffusion from a rod of length 1. \mia{ref to the problem somehow}
We utilized the PyTorch library \cite{Ansel_PyTorch_2_Faster_2024} \mia{check the citation, taken from previous project} for the neural network code. 
Adam is our optimizer of choice and has been used for all models. 
The weights have been initialized using a Kaiming normal distribution \mia{cite or ref}. 
Furthermore, we have specified a discretization of the $(x,t)$-grid, the batch size and the number of epochs \mia{more?}. 
The values for these are held constant and can be found in Tab. \ref{tab:valuesfornn}. 
% \mia{early stopping if we include it}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Epochs & 1000  \\ \hline
        Batch size & 3000 \\\hline
        Learning rate & 0.001 \\ \hline
        Number of poitns along x-axis (Nx) & 100 \\ \hline
        Number of poitns along t-axis (Nt) & 100 \\ \hline
    \end{tabular}
    \caption{\mia{make sure it is correct + write caption}}
    \label{tab:valuesfornn}
\end{table}

% \mia{initial layer and final layer size, train test split}
% \mia{how many iterations/seeds, choosing all seeds explicitly in order to be able to run parts of the code over again}

We wanted to find the best neural network model and explore the properties of some different hyperparameters. 
Firstly, we launched a three-dimensional grid search over the \textit{number of hidden layers}, the \textit{size of the hidden layers}, and the \textit{activation functions}. 
All possible combinations of the values given in Tab. \ref{tab:hyperparams} were used to train a model. 
This again is done for three different random seeds. \mia{different way of saying it?}
We compared the final output of the model to the analytical solution using mean squared error (MSE). 
The analytical solution was never used during training and was only utilized to evaluate the final models. 
We choose the best model in terms of MSE and keep track of the hyperparameters that produced it. 

To further explore the properties of the hyperparameters in Tab. \ref{tab:hyperparams}, we test all the other values of a hyperparameter while keeping the other two hyperparameter values constant as the best values from the grid search. 
We train each model ten times with different random seeds.
\mia{explain better}
For each of the three hyperparameters \textit{number of hidden layers}, \textit{size of hidden layers}, and \textit{activation functions}, we made boxplots with the different values of the hyperparameter along the x-axis and the MSE between the model and the analytical solution along the y-axis. \mia{megafixxxxx}


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Number of hidden layers & 1, 2, 3  \\ \hline
        Size of hidden layers & 10, 25, 50, 100 \\\hline
        Activation functions & tanh, ReLU, leaky ReLU, sigmoid \\ \hline
    \end{tabular}
    \caption{Hyperparameters used in the grid search for the best neural network model \mia{make sure it is correct}}
    \label{tab:hyperparams}
\end{table}
