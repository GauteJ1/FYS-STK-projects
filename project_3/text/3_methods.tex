For this report, we have computed the analytical solution to the heat equation for a rod of length $1$. 
Furthermore, we used the finite difference method and a neural network to find numerical solutions to the same problem. 
In this section, we describe our implementations of the aforementioned numerical methods. 

\subsection{Finite difference method}

We implement the finite difference method as an algorithm using a for-loop to "step" through each discrete time-point.
Using a discrete mesh with distance $\Delta x$ between nodes in the spatial dimension,
we have $N+1$ nodes in each time step.
The values for each time step is represented as a vector $u^n \in \mathbb{R}^{N+1}$.
\begin{align}
    \label{eq:vec_diff_scheme}
    u^{n+1} &= u^n + \frac{\Delta t}{\Delta x^2} D^{(2)}u^n \\
    \label{eq:diff_matrix}
    D^{(2)} &= 
    \begin{bmatrix}
    -2  & 1     &       &    \dots    &   0    \\
     1 & -2 & 1 &  &    \vdots    \\
     & \ddots & \ddots & \ddots & \\
     \vdots &     & 1 & -2 & 1 \\
     0&     \dots      &      & 1     & -2
    \end{bmatrix}
\end{align}
Solving Eq. \ref{eq:diff_scheme} for $u_j^{n+1}$, and vectorizing $u^n = (u^n_i)_{i=0}^N$, we get Eq. \ref{eq:vec_diff_scheme}.
The matrix $D^{(2)}$ is the second order differentiation matrix, defined as in equation \ref{eq:diff_matrix}.
Note that the $j$-th element in $D^{(2)}u^n$ is $u_{j+1}^n - 2u_n^j + u_{j-1}^n$ for $j = 1,2...,N$, matching the right hand side of Eq. \ref{eq:diff_scheme}.
In addition to this, we manually set the first and last element in each $u^n$ to zero at each step, in order to incorporate the boundary conditions.

The model takes as input the number of discrete elements in the $x$-direction, and calculates $\Delta x$ based on that.
We have implemented the function to either also take an input for $\Delta t$ and verifying that it is within the stability criterion, or, if no $\Delta t$ is given, to calculate and use the largest possible $\Delta t$ within the stability criterion.
While this finite difference scheme only requires us to save the previous time step, in order to calculate the next, we keep all time steps for plotting.

\subsection{Neural Networks}

We have trained several neural networks to predict the heat diffusion from a rod of length 1.
We utilized the PyTorch library \cite{Ansel_PyTorch_2_Faster_2024} for the neural network code. 
Adam was our optimizer of choice and has been used for all models. 
The weights were initialized using a Xavier Glorot normal distribution as described in Sec. \ref{sec:NN_init}. 
As the activation function for the output layer, $identity$ was used, i.e. no activation. 
Given the problem at hand, the size of the initial layer was $2$ and the output layer $1$. 
Furthermore, we specified a discretization of the $(x,t)$-grid, the batch size, and the number of epochs. 
The values for these hyperparameters are held constant across all models and can be found in Tab. \ref{tab:valuesfornn}. 
% \mia{early stopping if we include it}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Epochs & 1000  \\ \hline
        Batch size & 3000 \\\hline
        Learning rate & 0.001 \\ \hline
        Number of points along x-axis ($N_x$) & 100 \\ \hline
        Number of points along t-axis ($N_t$) & 100 \\ \hline
    \end{tabular}
    \caption{Values of hyperparameters that are common for all models}
    \label{tab:valuesfornn}
\end{table}

% \mia{initial layer and final layer size, train test split}
% \mia{how many iterations/seeds, choosing all seeds explicitly in order to be able to run parts of the code over again}

\mia{Synes dette er vanskelig Ã¥ forklare godt}

\julie{Julie}\mia{ og }\gaute{Gaute}\mia{!!!!!}

We wanted to find the best neural network and explore the properties of some different hyperparameters. 
First, we launched a three-dimensional grid search over the \textit{number of hidden layers}, the \textit{size of the hidden layers}, and the \textit{activation functions}. 
The size of the hidden layers dictates the width of the neural network, while the number of (hidden) layers gives the depth. 
All possible combinations of the values given in Tab. \ref{tab:hyperparams} were used to train a neural network. 
The same activation function was used for all hidden layers, which all had the same size. 
We compared the final output of the model to the analytical solution using mean squared error (MSE). 
The analytical solution was never used during training and only utilized to evaluate the final models. 
We chose the best model in terms of MSE and kept track of the hyperparameters that produced it. 

To further explore the properties of the hyperparameters in Tab. \ref{tab:hyperparams}, we trained more models. 
For one of the three hyperparameters in Tab. \ref{tab:hyperparams} at a time, we kept the the best value (found in the grid search) for the other two, and trained ten models for each of the values for the hyperparameter. 
I.e. for \textit{activation functions} we trained 10 models for each of the four different alternatives while keeping the width and the depth of the network to the best values. 


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Hyperparameter} & \textbf{Value}  \\ \hline
        Number of hidden layers & 1, 2, 3  \\ \hline
        Size of hidden layers & 10, 25, 50, 100 \\\hline
        Activation functions & tanh, ReLU, leaky ReLU, sigmoid \\ \hline
    \end{tabular}
    \caption{Hyperparameters used in the grid search for the best neural network model.}
    \label{tab:hyperparams}
\end{table}
