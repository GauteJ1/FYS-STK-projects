Linear regression models contribute a relatively simple, explainable and computationally efficient way to make data-driven decisions in comparison to more complex methods like neural networks.
However, these models do not necessarily perform well on all kinds of data.
Furthermore, their polynomial degree and hyper-parameters need to be tuned for each data set.
In this paper we implement three linear regression methods, ordinary least squares, Ridge, and Lasso regression, in order to explore their predicative abilities, both compared to each other, and relative to their respective adjustable parameters. 
We analyze the models using both synthetic data from the Franke function \citep[p. 13]{frank} as well as real terrain data \cite{mortengithub}.
We also explore the concept of bias-variance trade-off in the models, using resampling methods such as bootstrapping and k-fold cross-validation.
We conclude by reiterating the importance of selecting appropriate model complexities and values for model parameters, and suggest some guidance values for the relevant models and types of data. 
For the Franke function data, the optimal model is ordinary least squares, with polynomial degree 5 at an MSE of 0.0987, while the terrain data has optimal model ordinary least squares trained by 10-fold cross validation, polynomial degree 10 with 17.686 as MSE.
We find that linear regression is not optimal for the complex structure of terrain data. 