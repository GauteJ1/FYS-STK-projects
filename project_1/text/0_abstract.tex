Linear regression models contribute a relatively simple, explainable and computationally efficient way to make data-driven decisions in comparison to for example neural networks.
However, these models do not necessarily perform well on all kinds of data.
Furthermore, their complexity and hyper-parameters need to be tuned for each data set.
We implement linear regression models, including ordinary least squares-, Ridge- and Lasso regression, in order to explore their predicative abilities, both compared to each other, and relative to their respective adjustable parameters. 
Using both synthetic data from the Franke function \citep[p. 13]{frank} as well as real terrain data \cite{mortengithub}.
We also explore the concept of bias-variance trade off in the models, using resampling methods such as bootstrapping and k-fold cross-validation. 
We conclude by reiterating the importance of selecting appropriate model complexities and values for model parameters, and suggest some guidance values for the relevant models and types of data. 
For the Franke function data, the optimal model is ... at an MSE of ..., while the terrain data has optimal model ... with ... as MSE. \gaute{HEI!}
\gaute{is lin reg guud b√∏y?}