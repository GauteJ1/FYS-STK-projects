Linear regression models are widely used to make predictions on a wide range of data sets.
They contribute a relatively simple, explainable and computationally efficient way to make data-driven decisions in comparison to for example neural networks.
However, these models does not necessarily do a good job on any kind of data.
Furthermore, their complexity and hyper-parameters need to be tuned for each data set.
We implement linear regression models, including ordinary least squares regression, Ridge and Lasso, in order to explore their predicative abilities both compared to each other, and relative to their respective adjustable parameters. 
Using both synthetic data from the Franke function \citep[p. 13]{frank} as well as real terrain data from the U.S. Geological Survey. \mia{legge inn kilde for terrain data}
We also explore the concept of bias-variance trade off in the models, using resampling methods such as bootstrapping and k-fold cross-validation. 
We conclude by reiterating the importance of selecting appropriate model complexities and values for model parameters, and suggest some guidance values for the relevant models and types of data. \gaute{quote specific results}