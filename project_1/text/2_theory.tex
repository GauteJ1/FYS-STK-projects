\subsection{Linear Regression}

\julie{Must say that linear regression is linear in its parameters}

Linear regression is one of the most common methods for modeling a relationship between an input dataset and an output dataset.
Given a dataset of $p$ input variables (commonly called predictors), $X=[x_1, \, x_2, \, \ldots, \, x_p]$ and a dataset of output variables (commonly called the response) we seek a linear model on the form
\begin{equation}
\Tilde{\y}=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j},
\end{equation}
to model the relationship, where the scalar $\hat{Y}$ is our prediction of the response. 
Here $\beta_0$ is the intercept, and each $\beta_j$ is the coefficient belonging to its corresponding predictor $x_j$. \julie{konsistent bruk av beta og beta hat}

This equation is commonly written in vector form, 
\begin{equation}
\Tilde{y}=\boldsymbol{X}^T\boldsymbol{\hat{\beta}},
\end{equation}
or alternatively, $n$ sets of data points formatted in these parameters will be represented as a data set on the form
\begin{equation}
\Tilde{\y}=\boldsymbol{X}^T\boldsymbol{\hat{\beta}},
\end{equation}

\julie{mush into matrix form directly} 
where $\boldsymbol{\hat{Y}}$ is the response-vector and $\boldsymbol{X}$ is a $n\times p$ -matrix with each row an input-vector of predictors. \julie{design matrix}


As can be inferred by the equations above, linear regression bases itself on the assumption of a linear relationship between the predictors and the response. 
The ``true" model is assumed the form 
\begin{equation}\label{OG_y}
y=\beta_0+\sum_{j=1}^{p}x_j\beta_j+\epsilon,
\end{equation}
where $\epsilon$ is a \textit{error-term} or \textit{residual-term} representing all variance in the data not explainable by the linear model. 
Any variation due to randomness, noise or things like measurement error is included in this term. \julie{antar $\epsilon \sim  
\mathcal{N}(0,\sigma^2)$}
\julie{Nevne at vi aldri vil kunne kjenne de sanne beta-ene.}

\julie{Slenge inn uttrykkene for exp av $y_i$ og varians av $y_i$}

\subsection{Cost- \& Loss-Functions}
The main objective when solving such a linear regression problem, is finding the "optimal" coefficients $\boldsymbol{\beta}$ that minimizes the error-term. 
What such an optimal solution evinces is dictated by the definition of our \textit{cost-function},  or \textit{loss-function}, which is simply metrics chosen to measure how much our predictions deviate from the "truth". 
A cost-function, $\text{Cost}(f,\mathcal{D} )$, is used to describe such a metric measuring a group of data-points, while a loss-function, $L(y, \hat{y})$, describes a metric regarding a single data-instance. 
The cost-function can be expressed in terms of the loss function
\begin{equation}
\text{Cost}(f,\mathcal{D}) = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{y_i}).
\end{equation}
Through different choices of cost-function we end up with different methods for estimation and inference. 

%Depending on priorities when choosing, different models provide things like easier interpretation, lower \textit{bias}, or lower \textit{variance}. 

%https://www.baeldung.com/cs/cost-vs-loss-vs-objective-function

\julie{cite someone about the distinction cost/loss}


\subsection{Scaling of data}\label{scaling}

In order to have all predictors have an equal impact to the model, we need to scale the data \citep[p. 398]{hastie}. If we were to have one predictor that generally had values of order $10^3$, while another had values of order $10^{-3}$, the model could miss the predictive importance of the ladder predictor. This is due to the fact that we try to minimize the loss function in training. A deviation of a small number would count much less than deviation of larger value. Therefore, it is good practice to scale all data before training. 

A common choice \mia{cite} for scaling is to use the standard scaler. This method includes subtracting the mean and dividing by the standard deviation for each column. Each column will then have mean equal to zero and a standard deviation of one. 


We do not want to scale the intercept. \mia{cite + more} The intercept can be recalculated after the model fitting as Eq. \ref{bet0} shows.

\begin{equation}\label{bet0}
    \beta_0 = \frac{1}{n}\sum_{i=0}^{n-1}y_i - \frac{1}{n}\sum_{i=0}^{n-1}\sum_{j=1}^{p-1}X_{ij}\beta_j
\end{equation}

\subsection{Error estimation}

\subsubsection{Splitting data to avoid overfitting}\label{overfitting}

When training a model we want it to learn the underlying pattern of the data and not the specific training data. If we develop a too complex model that have learned the specific training data and perhaps even the noise, we have a case of overfitting. 

We therefore need two datasets, a set used for training and a set used for testing. We name these two datasets "training data" and "test data". 
We will use the latter to understand when the model starts learning beyond the underlying pattern. At this point, the training should be stopped. The test data is used for model selection.

In practice, we look at the error rate of the training and test data. The error on the training data will continue to decrease. For the test data, we will initially see a decrease in the error. After some time of training, this will increase again. At this point, the model is overfitting to the training data and we have our stopping criteria.

\plothere{Plot here of loss on training and test data to explain overfitting, as in Hastie}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{project_1/images/bias_var_bootstrap.png}\label{plot_overfit}
\caption{\mia{must insert the correct image}}
\label{train_test_overfit}
\end{figure}



\subsubsection{Error estimation}

The metric we truly want is the loss on independent data not seen or used in any way during training. Although the test data has not been trained on, we have used it to decide on when to stop the training. Our model is therefore not completely independent of the test data. Ideally one could have separated the data into three sets to overcome this. This is necessary to correctly choose between models, but only two sets are needed for training. The three datasets are typically called "training data", "validation data" and "test data". \mia{cite for paragraph}

\julie{Du må hjelpe meg her, Julie $\downarrow$}

The error measure we truly want is the prediction error on an independent test data set $\tau$: 
\begin{equation}\label{error_test}
    \text{Error}_{\tau} = \fv{L(\y,\hat{f}(\boldsymbol{x})) | \tau}
\end{equation}

The training error is not a good estimate of this. As explained in sec. \ref{overfitting} we can overfit to decrease this error measure. The training error is expressed as: 

\begin{equation}
    \overline{\text{err}} = \frac{1}{n}\sum_{i=0}^{n-1} L(y_i, \Tilde{y}_i)
\end{equation} 

\julie{Du må hjelpe meg her, Julie $\uparrow$}

\subsection{Cross-validation and bootstrapping}

Generally, we get a better model if we have more training data. \mia{cite?} When holding off some of the data for testing, we lose this data for training. We therefore want to explore methods that allow for a train-test split for the data, but still keep as much as possible for training. 

\subsubsection{K-fold cross-validation}
K-fold cross-validation is such a method. The number K must be chosen by the developer. We divide our dataset into K parts $\mathcal{F}_k$, called folds. K-1 folds are used as training data, while the remaining fold is used as test data. When the k-th fold is held out, we train model $f^{k}$. 
The procedure is repeated K times, holding out a new fold each time. The estimated error is then the mean of the K test errors as shown in Eq. \ref{cv}. \citep[p. 241]{hastie}


\begin{equation}\label{cv}
    CV_{error} = \frac{1}{K} \sum_{k=1}^{K} \frac{1}{|\mathcal{F}_k|} \sum_{i \in \mathcal{F}_k} L\left(y_i, f^{k}({x}_i)\right) 
\end{equation}

There are both advantages and disadvantages of this method. For a high K it is quite computationally costly. Another disadvantage is that there is no clear choice of K. There is a trade-off between bias and variance. A smaller K leads to lower variance, but higher bias. On the other side, a higher K leads to low bias, but high variance. The extreme case of the ladder is leave-out-one cross-validation (LOOCV). It will lead to an unbiased error measure, but the variance will be quite large \citep[p. 242]{hastie}.

One advantage of cross-validation is that more data is left for training. Another advantages is that the error (Eq. \ref{cv}) closer to the true generalization error measure (Eq. \ref{error_test} )because we average over many different models. \mia{fill in ones prev sec is done by} \julie{Julie}


\subsubsection{Bootstrapping}
Bootstrapping is a method where we draw with replacement from the original data to have a new data set for training. Every bootstrap sample should have the same size as the original data set, $n$. We generate many such samples and repeat the training. The total number of samples is denoted by B.
%This is meant to simulate new experiments.\mia{cite} 
For the b-th bootstrap sample we obtain model $\hat{f}_b$ \citep[p. 249]{hastie}.

The error could calculated as the mean of the error:

\begin{equation}
    \widehat{\text{Err}}^{(1)} = \frac{1}{n} \sum_{i=0}^{n-1} \frac{1}{|C_{[-i]}|} \sum_{b \in C_{[-i]}} L\left(y_i; \hat{f}_b(x_i)\right)
\end{equation}

However, this might be too optimistic \citep[p. 251]{hastie}. On average we have 63.2\% of the original observations in a bootstrap sample. This corresponds to 36.8\% not being in the bootstrap sample at all. \mia{more} This correct error measure for bootstrapping is therefore: 

\begin{equation}
    BS_{error} = 0.368 \overline{\text{err}} + 0.632 \widehat{\text{Err}}^{(1)}
\end{equation}


\subsection{Ordinary least squares}
Ordinary least squares provide an unbiased estimation - meaning the expected distance between our prediction and the truth is zero. 
\julie{utdyp bias med ligninger osv, kanskje dette sist egt og cost funksjon først}

\begin{equation}\label{betaols}
    \hat{\bet}_{\text{OLS}} = \betta
\end{equation}

\begin{equation}
    \fv{y_i} = \mathbf{X}_{i,*}\boldsymbol{\beta}
\end{equation}

\begin{equation}
    \text{var}(y_i) = \sigma^2
\end{equation}

\begin{equation}
    \fv{\hat{\bet}} = \bet
\end{equation}

\begin{equation}
   \text{var}(\hat{\bet}) = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\end{equation}

%\mia{Could discuss a bit the advantages and disadvantages of the two compared to each other. Ridge best for prediction, lasso best for model selection. Could link to AIC and BIC, but probably just messy to include to more measures. Could discuss that we could have methods that are combinations of the two $\rightarrow$ elastic net. There is no gradient for lasso, but there is for ridge. Must mention here or somewhere else that it is especially important to scale when we have a penalty because the penalty is not scale invariant.}

%\mia{I think it could be useful with some figures in the theory, showing bias, variance, the scaling of the parameters in ride and lasso, etc. Could use either size of beta against lambda or the geometric variant. }

\subsection{Penalized linear regression methods}

An extension of the ordinary least squares method is to add a penalization term. A general term for this is shrinkage methods. There are many reasons why this is often preferred. \mia{cite for the last statement}

Firstly, to perform OLS we assume that the matrix $\boldsymbol{X}^T\boldsymbol{X}$ in Eq. \ref{betaols} is invertible. This may not always be the case due to correlation between the predictors in the data set or if $p > n$. Then the matrix will not be full rank. 

A mathematical fix to this is to add a (small) number along the diagonal: 
\begin{equation}\label{pen}
    \bet = (\mathbf{X}^T\mathbf{X}- \lambda\boldsymbol{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}

Secondly, these methods also reduce overfitting. \mia{cite (Morten)}

Eq. \ref{pen} is the general equation for penalized regression. The parameter $\lambda$ controls the regularization. Furthermore, we have different types of penalties. Two types are the L1-norm penalty, also known as Lasso, and the L2-norm penalty, known as Ridge. We will see that different types of penalties give different properties and interpretations.

\subsubsection{Ridge Regression}\label{ridge_sec}

In Ridge regression, an L2-norm penalty is used. This amounts to the following cost-function (Eq. \ref{ridge}) and constraint on the $\beta$'s (Eq. \ref{ridge_constraint}):

\begin{equation}\label{ridge}
     C(\bet) = \frac{1}{n} \sum_{i=0}^{n-1} \left( y_i - \sum_{j=1}^{p-1} X_{ij}\beta_j \right)^2 + \lambda\sum_{j=1}^{p-1} \beta_j^2 
\end{equation}



\begin{equation}\label{ridge_constraint}
    \sum_{j=1}^p \beta_j^2 \leq t, 
\end{equation}

The value of t is directly related to the value of $\lambda$ \citep[p. 63]{hastie}.

The solutions produced by Ridge regression depend on the scaling of the data. It is therefore especially important to standardize the data as explained in sec. \ref{scaling}. Ridge regression puts a penalty on all the $\beta$-terms except the intercept which is held out during training. If it had been included, the model would depend on the chosen origin \citep[p. 63]{hastie}.
The value of $\beta_0$ is later calculated as Eq. \ref{bet0} shoes. 

In ridge regression, the values of $\beta_j$ are forced closer to zero, but can never be zero completely. \mia{consequences}

It can be showed that $\bet_{OLS}$ and $\bet_{Ridge}$ are related as shown in Eq. \ref{ridgeOLS}. 

\begin{equation}\label{ridgeOLS}
    \bet_{Ridge} = \frac{1}{1+\lambda}\bet_{OLS}
\end{equation}




\subsubsection{Lasso Regression}

Lasso regression is another shrinkage method. This method uses an L1-norm penalty. We get the cost function and constraint on $\beta$ as Eq. \ref{lasso} and Eq. \ref{lasso_constraint} respectively shows. 

\begin{equation}\label{lasso}
     C(\bet) = \frac{1}{n} \sum_{i=0}^{n-1} \left( y_i - \sum_{j=0}^{p-1} X_{ij}\beta_j \right)^2 + \lambda\sum_{j=0}^{p-1} |\beta_j|
\end{equation}

\begin{equation}\label{lasso_constraint}
    \sum_{j=1}^p | \beta_j | \leq t, 
\end{equation}


The value of t is directly related to the value of $\lambda$ \citep[p. 68]{hastie}.


As in Ridge regression, the intercept is held out. The same reasoning holds for Lasso regression and the reader is refereed to sec. \ref{ridge_sec}.

In Lasso regression the value of the $\beta$'s can be out to zero. If the value of $\lambda$ is suffeciently large, we \mia{continue}

\mia{same as \ref{ridgeOLS} for Lasso?}

\subsubsection{Comparison}

\plothere{Sirkel og diamant}

\subsection{Bias-Variance Tradeoff}

\julie{JULIE}

\subsubsection{Bias}
The bias of an estimator is the difference between the expected value of the estimator and its true value as shown in Eq. \ref{bias}. 

\begin{equation}\label{bias}
    \text{Bias}(\hat{\bet}) = \fv{\hat{\bet}}-\bet
\end{equation}

The bias is the distance between the actual target and the prediction. If a model is overfitted the bias is low. The predictions are very close to the target in the training data set. An underfitted model however, has a high bias. In both cases, we have a model that does not capture the real underlying pattern but is rather too complex or simple respectively. 

High bias entails a systematic miss of the true pattern in the data. It can easily be understood by looking at an archery target \plothere{Add a figure after variance and explain all}. 

\subsubsection{Variance}

The variance of an estimator is its sensitivity to fluctuations in the data sets. If the predictions vary a lot depending on which data set we train on, it has high variance. The equation for the calculation of the quantity is shown in Eq. \ref{var}. 

\begin{equation}\label{var}
    \text{Var}(\hat{\bet}) = \fv{\hat{\bet}-\fv{\hat{\bet}}^2} = \fv{\hat{\bet}^2} - \fv{\hat{\bet}}^2
\end{equation}

An overfitted model generally has high variance. \mia{cite?} As the model learns the specific training data and not the underlying pattern, it is reasonable that the model becomes very sensitive to the specific training data. 

\subsubsection{The trade-off}

\mia{mention that we here use OLS, but that the concepts hold in general}

During training the objective is to minimize the loss function.  We can decompose the expression for the expected loss. Calculations are available in \hyperref[appendixB]{appendix B}.

\begin{equation}\label{biasvar}
    \fv{(\y - \yt)^2} = \text{Bias}(\yt)^2 + \text{var}(\yt) + \sigma^2
\end{equation}

Eq. \ref{biasvar} shows how the expected loss is decomposed into a bias term, a variance term, and an irreducible error. The latter comes from $\epsilon$ in Eq. \ref{OG_y}. As the term irreducible error suggests, there is nothing for us to do with this term. We are therefore left with a bias term and a variance term. These terms are inversely related and in our to have the smallest \mia{error/loss} in total, we need to balance them. 

An overfitted model has low bias, but high variance. An underfitted model has the opposite, high bias and low variance. In fig. \ref{train_test_overfit} this corresponds to high bias and low variance in to the left and the opposite to the right. 


\subsection{Evaluation measures}

Mean squared error, hereby MSE, is defined as: 

\begin{equation}\label{mse}
    \text{MSE} = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\Tilde{y}_i)^2
\end{equation}

The number of data points in our training data is $n$ while the prediction for the i-th data point is denoted $\Tilde{y}_i$. MSE is a popular \mia{cite} error measure for linear regression models. 

\begin{equation}
    R^2 = 1 - \frac{\sum_{i=0}^{n-1}(y_i-\Tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i-\overline{y}_i)^2}
\end{equation}

$R^2$ is a measure of how much of the variance in the data the model explains. \mia{cite} Furthermore $R^2 \in [0,1]$. If $R^2 = 1$ the model perfectly explains all variance, whereas a value of 0 would mean does not explain any of the variance. The numerator is the sum of the squared residuals, also called RSS. The denominator is the total sum of squares, in short TSS. 

