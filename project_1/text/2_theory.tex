\subsection{Linear Regression}
Linear regression bases itself on the assumption of a linear relationship between the \textit{predictors} and the \textit{response} \cite[p.21-26]{fahrmeir}. 
Given a dataset of $p$ input variables (commonly called predictors), $X=[x_1, \, x_2, \, \ldots, \, x_p]$ and a dataset of output variables (commonly called the response) we seek a linear model on the form
\begin{equation}
\Tilde{y}=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j}.
\end{equation}
The scalar $\Tilde{y}$ is our prediction of the response, $\hat{\beta_0}$ the estimated intercept, and each $\hat{\beta_j}$ the estimated coefficient belonging to its corresponding predictor $x_j$. 

This equation is commonly written in vector form, 
\begin{equation}
\Tilde{\y}=\boldsymbol{X}^T\boldsymbol{\hat{\beta}},
\end{equation}

where given n different sets of input/output-variables, $\Tilde{\y}$ is the response-vector and $\boldsymbol{X}$ is a $n\times (p+1)$-matrix called the \textit{design matrix}\label{design-matrix}. Here the $'+1'$ column in $\boldsymbol{X}$ is a row of ones for inclusion of the intercept in $\boldsymbol{\hat{\beta}}$, and the residual $p$ columns each of the p predictor variables. 


The ``true" model is assumed the form 
\begin{equation}\label{OG_y}
y=\beta_0+\sum_{j=1}^{p}x_j\beta_j+\epsilon,
\end{equation}
Our linear regression models are always estimations of the equation above, and we have no way of knowing the true $\boldsymbol{\beta}$. 
$\epsilon$ is some irreducible \textit{error-term} or \textit{residual-term} representing all variance in the data not explainable by the linear model; any variation due to randomness is included in this term. It is assumed $\epsilon \sim  
\mathcal{N}(0,\sigma^2)$, and given this assumption we get the expected value of $y_i$

\begin{equation}\label{expected_y}
    \fv{y_i} = \mathbf{X}_{i,*}\boldsymbol{\beta},
\end{equation}
and the variance of $y_i$
\begin{equation}\label{var_y}
    \text{var}(y_i) = \sigma^2.
\end{equation}



Calculations of Eq. \ref{expected_y} and \ref{var_y} are available in \hyperref[appendixB]{appendix B}.

\subsection{Cost- \& Loss-Functions}
The main objective when solving such a linear regression problem, is finding the optimal coefficients $\boldsymbol{\beta}$ that minimizes an error measure between $y_i$ and $\Tilde{y_i}$. 
How such an optimal solution evinces is dictated by the definition of our \textit{cost-function},  or \textit{loss-function}, which is simply metrics chosen to measure how much our predictions deviate from the "truth". 
A cost-function, $\text{Cost}(f,\mathcal{D} )$, is used to describe such a metric measuring a group of data-points, while a loss-function, $L(y, \hat{y})$, describes a metric regarding a single data-instance. 
The cost-function can be expressed in terms of the loss function
\begin{equation}
\text{Cost}(f,\mathcal{D}) = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{y_i}),
\end{equation}
as the average of the loss-function over the data. 
Through different choices of cost-function we end up with different methods for estimation, resulting in different models for the same data-set. 

%Depending on priorities when choosing, different models provide things like easier interpretation, lower \textit{bias}, or lower \textit{variance}. 

%https://www.baeldung.com/cs/cost-vs-loss-vs-objective-function


\subsection{Data handling}

\subsubsection{Scaling}\label{scaling}

In order for all parameters to have an equal starting point when training a model, it's often necessary to scale the data \citep[p. 398]{hastie}. Having one parameter with values of order $10^3$, and another parameter with values of order $10^{-3}$, the model could miss the predictive importance of the latter. Whether or not it is necessary to scale the data, depends on the choice of cost-function; Given two parameters of different magnitudes where the same relative change has equal impact on the response, the parameter of smaller magnitude would have coefficients of inversely proportional magnitude. If a cost-function penalizes bigger coefficients, as some does, the parameter of smaller magnitude would then be penalized more than it should. To alleviate the risk of such problems, it is thus necessary for many methods that we scale all data before training. 

A common choice for scaling is to use the \textit{standard scaler}. This method involves subtracting the mean and dividing by the standard deviation, for each feature (i.e. parameter) respectively. Each of the columns will then have mean equal to zero and a standard deviation of one after the scaling. For the i-th datapoint and the j-th feature we get the transformation as indicated in Eq. \ref{standardize} \citep[Linear Regression]{morten}. Alternatively a variation of the standard scaler is sometimes used, including only subtraction of the mean and omitting the division of standard deviation.

\begin{equation}\label{standardize}
    x_{ij} \rightarrow \frac{x_{ij}-\overline{x}_j}{\sigma_{x_j}}
\end{equation}

It's considered good practice to not scale the intercept, as it represents a constant term without variability and scaling it will often worsen it's performance, or defeat it's purpose entirely. It can be taken out during training and later be recalculated as Eq. \ref{bet0} shows \citep[Resampling methods]{morten}. We will later touch on how some methods would produce different results if a scaled intercept was included in the training. Taking out the intercept amounts to removing the column of 1's in the design matrix. 

\begin{equation}\label{bet0}
    \beta_0 = \frac{1}{n}\sum_{i=0}^{n-1}y_i - \frac{1}{n}\sum_{i=0}^{n-1}\sum_{j=1}^{p-1}X_{ij}\beta_j
\end{equation}

\subsubsection{Splitting data into sets}\label{overfitting}

When training a model we want it to learn the underlying pattern of the data that is (hopefully) representative for the entire population the data is sampled from. 
Testing a model on the same data it's trained on could give a false impression of it's accuracy; a model that takes into account variation specific to the data it's trained on (that's not representative for the larger population) would get rewarded in this testing, while a model that does not take it into account would get punished. 

We therefore need (at least) two datasets; a set used for training and a set used for testing, to give us a more impartial evaluation of the model. The two datasets are named respectively "training data" and "test data". As can be inferred by the names, we use the training data in the training (or \textit{fitting}) of the model, while the test set is used to evaluate the performance of our final chosen model. 
Additionally one can choose to include a "validation" set in the split; this validation set would then be applied in the model selection phase.

An important concept in train-test splitting is to \textbf{never} touch the test-set before the final evaluation. The test-set is supposed to act as never-before seen data for the model, and given a limited data set it's the closest one gets to an impartial assessment. 
If using a scaler, fitting of the scaler should be done after the split and solely on the training data - and this scaler, fit to the training data, then applied to the test data when testing. 

Splitting data is an important tool to help avoid \textit{overfitting} and \textit{underfitting} of the model, and ensure a more generalizable fit. Overfitting and underfitting is elaborated further in  section G; "\textit{Bias-Variance Tradeoff}".
\julie{KILDER}
% We will use the latter to understand when the model starts learning beyond the underlying pattern. At this point, the training should be stopped. The test data is used for model selection.

% If we develop a too complex model that gets specialized to the training data and perhaps even noise, we get a case of \textit{overfitting}. 

% In practice, we look at the error rate of the training and test data. The error on the training data will continue to decrease. For the test data, we will initially see a decrease in the error. After some time of training, this will increase again. At this point, the model is overfitting to the training data and we have our stopping criteria.

%\plothere{Plot here of loss on training and test data to explain overfitting, as in Hastie}

% \begin{figure}[h!]
% \centering
% \includegraphics[width=1\linewidth]{project_1/figures/bias_var_bootstrap.png}\label{plot_overfit}
% \caption{\mia{must insert the correct image}}
% \label{train_test_overfit}
% \end{figure}



%\subsubsection{Error estimation}

%The metric we truly want is the loss on independent data not seen or used in any way during training. Although the test data has not been trained on, we have used it to decide on when to stop the training. Our model is therefore not completely independent of the test data. Ideally one could have separated the data into three sets to overcome this. This is necessary to correctly choose between models, but only two sets are needed for training. The three datasets are typically called "training data", "validation data" and "test data". \mia{cite for paragraph}

%\julie{Du må hjelpe meg her, Julie $\downarrow$}

%The error measure we truly want is the prediction error on an independent test data set $\tau$: 
%\begin{equation}\label{error_test}
    %\text{Error}_{\tau} = \fv{L(\y,\hat{f}(\boldsymbol{x})) | \tau}
%\end{equation}

%The training error is not a good estimate of this. As explained in sec. \ref{overfitting} we can overfit to decrease this error measure. The training error is expressed as: 

%\begin{equation}
    %\overline{\text{err}} = \frac{1}{n}\sum_{i=0}^{n-1} L(y_i, \Tilde{y}_i)
%\end{equation} 

%\julie{Du må hjelpe meg her, Julie $\uparrow$}

\subsection{Resampling methods}

When training models, error estimation is crucial both for model selection and model assessment. Without a good metric for the performance of the model it is impossible to gauge how well fit a model actually is.
Generally, we want as much data as possible for the training of a model. When holding off some of the data for testing, we lose this data for training. It's therefore of interest to explore methods that allow for a train-test split of the data, while still keeping as much data as possible for training and giving us a better error estimation.

\subsubsection{K-fold cross-validation}
\textit{K-fold cross-validation} is one such resampling method. The $K$ represents a number that must be chosen by the developer as they see fit. We divide our dataset into $K$ parts $\mathcal{F}_k$, called folds. $K-1$ folds are used as training data, while the remaining fold is used as test data. The model trained when the $k$-th fold is held out, is denoted $f^{k}$. 
The procedure is repeated $K$ times, holding out a new fold each time. For k-fold cross-validation the estimated error is given as the mean of the $K$ test errors, shown in Eq. \ref{cv}. \citep[p. 241]{hastie}


\begin{equation}\label{cv}
    CV_{error} = \frac{1}{K} \sum_{k=1}^{K} \frac{1}{|\mathcal{F}_k|} \sum_{i \in \mathcal{F}_k} L\left(y_i, f^{k}({x}_i)\right) 
\end{equation}

There are both advantages and disadvantages of this method. On one hand, it achieves the goal of maintaining a train-test split while keeping a sizeable train-set. 
The estimated error (Eq. \ref{cv}) will be closer to the true generalization error measure. This is a consequence of the error being averaged over many different models, and thereby to a higher degree taking into account randomness associated with data-selection. \mia{cite} \julie{KILDER}

On the other hand, cross-validation will be quite computationally costly for a high $K$. Another disadvantage is that there is no clear choice of $K$; here there is a trade-off between bias and variance. A smaller K leads to lower variance, but higher bias. On the other side, a higher K leads to low bias, but high variance. The extreme case of the ladder is leave-out-one cross-validation (LOOCV), where every datapoint acts as a fold. It will lead to an unbiased error measure, but the variance will be quite large \citep[p. 242]{hastie}.

\julie{KILDER}

\subsubsection{Bootstrapping}
Bootstrapping is a general term for another method of resampling where one draws with replacement from the original dataset, creating a "new" dataset for each bootstrap. Every bootstrap sample should have the same size as the original data set, $n$. We generate many such samples and repeat the training for each of them. The total number of samples is denoted by B. 
For the b-th bootstrap sample we obtain model $\hat{f}_b$ \citep[p. 249]{hastie}.

Using bootstrap we get a possible estimation of the error calculated as the mean of the error across the $B$ bootstrap samples \cite[p. 250]{hastie}:

\begin{equation}
    BS_{error} = \frac{1}{n}\frac{1}{B}\sum_{i=0}^{n-1}\sum_{b=1}^{B} L\left(y_i, \hat{f}_b(x_i)\right)
\end{equation}
\

% \begin{equation}
%    \widehat{\text{Err}}^{(1)} = \frac{1}{n} \sum_{i=0}^{n-1} \frac{1}{|C_{[-i]}|} \sum_{b \in C_{[-i]}} L\left(y_i; \hat{f}_b(x_i)\right)
% \end{equation}

%However, this might be too optimistic. On average we have 63.2\% of the original observations in a bootstrap sample. This corresponds to 36.8\% not being in the bootstrap sample at all. \mia{more} This correct error measure for bootstrapping is therefore \citep[p. 251]{hastie}: 

%\begin{equation}
    %BS_{error} = 0.368 \overline{\text{err}} + 0.632 \widehat{\text{Err}}^{(1)}
%\end{equation}

\subsection{Evaluation measures}

The \emph{mean squared error} (MSE) is a popular error measure for linear regression models, and is defined as: 

\begin{equation}\label{mse}
    \text{MSE} = \frac{1}{n}\sum_{i=0}^{n-1}(y_i-\Tilde{y}_i)^2
\end{equation}

$n$ denotes the number of data points in our training data, and the prediction for the i-th data point is denoted $\Tilde{y}_i$.


$R^2$ is a measure of how much of the variance in the data the model explains. %\mia{cite} 
\begin{equation}
    R^2 = 1 - \frac{RSS}{TSS}= 1 - \frac{\sum_{i=0}^{n-1}(y_i-\Tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i-\overline{y}_i)^2},
\end{equation}

$R^2 \in [0,1]$. If $R^2 = 1$ the model perfectly explains all variance, whereas a value of 0 would mean does not explain any of the variance. The numerator is the sum of the squared residuals, also called RSS. The denominator is the total sum of squares, in short TSS. \mia{cite}

\subsection{Ordinary least squares}

\textit{Ordinary least squares} is a linear regression method where we seek to minimize the following cost function:

\begin{equation}\label{cost_ols}
    C(\bet) = \sum_{i=0}^{n-1}(y_i-\Tilde{y}_i)^2
\end{equation}

Comparing the expression above to Eq. \ref{mse}, we see how the OLS regression method is inherently designed to minimize MSE.

From Eq. \ref{cost_ols} we can derive the equation for the optimal $\bet$. This is done by taking the derivate of the cost function wrt. $\bet$ and finding the minimum. The optimal $\bet$ for OLS is shown in Eq. \ref{betaols}. 

\begin{equation}\label{betaols}
    \hat{\bet}_{\text{OLS}} = \betta = \boldsymbol{H}\y
\end{equation}

The $\boldsymbol{H}$ is popularly called the Hessian matrix. The Hessian matrix for OLS specifically is stated in Eq. \ref{betaols}, but the term "Hessian matrix" generally describes a square matrix of double derivatives.


Ordinary least squares provides an unbiased estimation - meaning the expected distance between our prediction and the truth is zero. 

\begin{equation}
    \fv{\hat{\bet}_{OLS}} = \bet
\end{equation}

\begin{equation}
   \text{var}(\hat{\bet_{OLS}}) = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\end{equation}

OLS regression is invariant to scaling of the data.

%\mia{Could discuss a bit the advantages and disadvantages of the two compared to each other. Ridge best for prediction, lasso best for model selection. Could link to AIC and BIC, but probably just messy to include to more measures. Could discuss that we could have methods that are combinations of the two $\rightarrow$ elastic net. There is no gradient for lasso, but there is for ridge. Must mention here or somewhere else that it is especially important to scale when we have a penalty because the penalty is not scale invariant.}

%\mia{I think it could be useful with some figures in the theory, showing bias, variance, the scaling of the parameters in ride and lasso, etc. Could use either size of beta against lambda or the geometric variant. }

\subsection{Penalized linear regression methods}

An extension of the ordinary least squares method is to add a penalization term to the cost-function. There are many reasons why this is often preferred. 

Firstly, to perform OLS we assume that the matrix $\boldsymbol{X}^T\boldsymbol{X}$ in Eq. \ref{betaols} is invertible. This may not always be the case due to correlation between the predictors in the data set, or if $p > n$. In these cases the matrix will not be full rank, i.e. not invertible. 
A mathematical fix to this is to add a (small) number $\lambda$ along the diagonal: 
\begin{equation}\label{pen}
    \bet = (\mathbf{X}^T\mathbf{X}- \lambda\boldsymbol{I})^{-1}\mathbf{X}^T\mathbf{y}
\end{equation}

These methods also help to reduce overfitting. \mia{cite (Morten)} In a sense we're normally penalizing "too good of a fit", meaning it's harder for models too get to specialized to specifics in the training data.

Eq. \ref{pen} is the general equation for the coefficients in penalized regression, where the parameter $\lambda$ controls the regularization. Among the many types of choices for penalization metrics we have the L1-norm penalty, also known as Lasso, and the L2-norm penalty, known as Ridge. Different types of penalties yields in different properties and interpretations related to the resulting models.

\subsubsection{Ridge Regression}\label{ridge_sec}

In Ridge regression, the L2-norm penalty gives us the following cost-function (Eq. \ref{ridge}):

\begin{equation}\label{ridge}
     C(\bet) = \sum_{i=0}^{n-1} \left( y_i - \sum_{j=1}^{p-1} X_{ij}\beta_j \right)^2 + \lambda\sum_{j=1}^{p-1} \beta_j^2 
\end{equation}




This can alternatively be expressed as Eq. \ref{ridge_alt} and \ref{ridge_constraint}, where the restraint on $\bet$ is explicitly stated. Here the value of t is directly related to the value of $\lambda$ \citep[p. 63]{hastie}.
\begin{equation}\label{ridge_alt}
    \sum_{i=1}^N \left(y_i - \sum_{j=1}^{p-1} X_{ij}\beta_j \right)^2
\end{equation}
\begin{equation}\label{ridge_constraint}
    \sum_{j=1}^p \beta_j^2 \leq t, 
\end{equation}

Ridge regression puts a penalty on all the $\beta$-terms except the intercept, which is held out during training. Had it been included, the model would depend on the chosen origin and this dependency undermines the principle of shift invariance \citep[p. 63]{hastie}.
The value of $\beta_0$ is later calculated as Eq. \ref{bet0} shows. 


It can be showed that $\bet_{OLS}$ and $\bet_{Ridge}$ are related as shown in Eq. \ref{ridgeOLS}. \julie{bare hvis }\mia{X er ortonormal?}

\begin{equation}\label{ridgeOLS}
    \bet_{Ridge} = \frac{1}{1+\lambda}\bet_{OLS}
\end{equation}

The solutions produced by Ridge regression depend on the scaling of the data. It is therefore especially important to standardize the data as explained in sec. \ref{scaling}.



\subsubsection{Lasso Regression}

Lasso regression is another penalization method. This method uses an L1-norm penalty, giving us the cost function and constraint on $\beta$ as Eq. \ref{lasso}. 

\begin{equation}\label{lasso}
     C(\bet) = \sum_{i=0}^{n-1} \left( y_i - \sum_{j=0}^{p-1} X_{ij}\beta_j \right)^2 + \lambda\sum_{j=0}^{p-1} |\beta_j|
\end{equation}

Similar to Ridge regression, there is an alternative formulation, Eq. \ref{lasso_constraint} and Eq. \ref{lasso_constraint}, explicitly stating the constraint on $\bet$;
\begin{equation}\label{lasso_alt}
    \sum_{i=1}^N \left(y_i - \sum_{j=1}^{p-1} X_{ij}\beta_j \right)^2
\end{equation}
\begin{equation}\label{lasso_constraint}
    \sum_{j=1}^p | \beta_j | \leq t, 
\end{equation}


The value of t is again directly related to the value of $\lambda$ \citep[p. 68]{hastie}.


The intercept is held out in training, by the same reasoning as for Ridge regression and the reader is refereed to sec. \ref{ridge_sec}.


\subsubsection{Ridge vs. Lasso}
Both Ridge and Lasso are known as \textit{shrinkage methods}, as they both shrink the coefficients. Lasso additionally goes under the term \textit{selection method}, as the coefficients can be shrunk to zero resulting in only a selection of the features contributing in the model. 
If the value of $\lambda$ is sufficiently large, we will see that more coefficients become zero. 
In ridge regression, the values of $\beta_j$ are forced closer to zero, but can never be zero completely. 

The two methods, specifically the difference in how the coefficients approach zero, are illustrated in Fig. \ref{fig:ridge_lasso}. Both methods will find a solution where the error function intersects the constraint. For the circular ridge constraints, this intersection will never coincide with the axis i.e. the coefficients will always be non-zero. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{project_1/figures/ridge-lasso.png}
    \caption{Illustration of the optimal coefficients in ridge regression (left) and lasso regression (right). The blue areas illustrate the constraints given in respectively Eq. \ref{ridge_constraint} and Eq. \ref{lasso_constraint}, and the red ellipses contours of a least squares error function. \cite[Taken from][]{hastie}}
    \label{fig:ridge_lasso}
\end{figure}



\subsection{Bias-Variance Tradeoff}

\subsubsection{Bias}
The bias of a method (or estimator) describes its inability to capture the true relationship we're estimating. For a specific model this a measure of the difference between the expected values of the estimator and the true values, as shown in Eq. \ref{bias}. 

\begin{equation}\label{bias}
    \text{Bias}(\hat{\bet}) = \fv{\hat{\bet}}-\bet
\end{equation}

If we were to train models with the same method on different datasets, models with low bias would all on average predict close to the target values they're trained on. Models with high bias would on the contrary predict values further from the true target. Bias is often described using shots at an archery target, where each shot represents a model trained on different data; high bias is analogous to shots far from the bulls-eye (which is our goal), while lower bias to shots closer to the bulls-eye.
% The predictions are very close to the target in the training data set. An underfitted model however, has a high bias. In both cases, we have a model that does not capture the real underlying pattern but is rather too complex or simple respectively. 
%It can easily be understood by looking at an archery target \plothere{Add a figure after variance and explain all}. 

\subsubsection{Variance}

The variance of a model describes the amount of variation between it's predictions and the expected value of the predictions, as shown in Eq. \ref{var}. This in turn explains a models sensitivity to fluctuations in the data sets; for a model with high variance the predictions will vary a lot depending on which data set we train on, and vice versa.

\begin{equation}\label{var}
    \text{Var}(\hat{\bet}) = \fv{\hat{\bet}-\fv{\hat{\bet}}^2} = \fv{\hat{\bet}^2} - \fv{\hat{\bet}}^2
\end{equation}

Relating back to the analogy of a shots at a target, high variance relates to shots that deviate a lot from one another, while lower variance to shots more closely grouped together.  
% An overfitted model generally has high variance. As the model learns the specific training data and not the underlying pattern, it is reasonable that the model becomes very sensitive to the specific data it is tested on.

\subsubsection{The trade-off}

%\mia{mention that we here use OLS, but that the concepts hold in general}

When evaluating statistical models we often talk about the expected error of our model, as a measure of how good our model is. This expected error can be decomposed as shown in Eq. \ref{biasvar}. 

\begin{equation}\label{biasvar}
    \fv{(\y - \yt)^2} = \text{Bias}(\yt)^2 + \text{var}(\yt) + \sigma^2
\end{equation}

Calculations are available in \hyperref[appendixB]{appendix B}, here specifically for a model fit with ordinary least squares (OLS) regression. 
Eq. \ref{biasvar} gives us the expected error composed of a model-bias term, a model-variance term and lastly the variance of $\epsilon$; the irreducible error-term as given in Eq. \ref{OG_y}. As the latter of the three is irreducible and out of our control, we are therefore left with the bias term and variance term to try and reduce for optimizing our models. 
The ideal model would have both low bias and low variance, but as the two terms are inversely related - this is highly improbable. Instead we seek to find the optimal balance between them - this is called the \textit{bias-variance trade-off}. 

The edge cases of this trade-off are usually denoted overfitting and underfitting (as mentioned in Section \ref{overfitting}). Overfitted models have low bias and high variance, while an underfitted models have the opposite; high bias and low variance. 

An overfitted model is characterized as being too complex; often having too many parameters, degrees of freedom or similar measures of complexity. 
Overfit models pick up on all small variations in training data, including those caused by noise and randomness. As a consequence these models perform excellent on their training data, but are expected to perform quite terribly on new data sets. 

An underfit model is contrastingly characterized as being too simple. Underfit models are not very specialized to the training data, failing to pick up on variations related to the underlying distribution of the data. As a consequence these models perform similarly on training data and test data, but this performance is not very good. 


\julie{ref morten uke37, statquest kanskje}