\subsection{Linear Regression}
Linear regression is one of the most common methods for modelling a relationship between an input dataset and an output dataset.
Given a dataset of $p$ input variables (commonly called predictors), $X=[x_1, \, x_2, \, \ldots, \, x_p]^T$ and a dataset of output variables (commonly called the response) we seek a linear model on the form
\[
\hat{Y}=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j},
\]
to model the relationship, where the scalar $\hat{Y}$ is our prediction of the response. 
Here $\beta_0$ is the intercept, and each $\beta_j$ is the coefficient belonging to its corresponding predictor $x_j$. 
This equation is commonly written in vector form, 
\[
\hat{Y}=\boldsymbol{X}^T\boldsymbol{\beta},
\]
or alternatively $n$ sets of data-points formatted in these parameters will be represented as a data-set on the form
\[
\boldsymbol{\hat{Y}}=\boldsymbol{X}^T\boldsymbol{\beta},
\]
where $\boldsymbol{\hat{Y}}$ is the response-vector and $\boldsymbol{X}$ is a $n\times p$ -matrix with each row an input-vector of predictors.

\julie{$\uparrow$ Her er jeg kollosalt usikker på rad vs kolonne osv, pls check.}

As can be inferred by the equations above, linear regression bases itself on the assumption of a linear relationship between the predictors and the response.
The ``true" model is assumed the form 
\[
Y=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j}+\epsilon,
\]
where $\epsilon$ is a \textit{error-term} or \textit{residual-term} representing all variance in the data not explainable by the linear model. 
Any variation due to randomness, noise or things like measurement-error is included in this term. 
\subsection{Cost-Functions \& Loss-Functions}
The main objective when solving such a linear regression problem, is finding the ``optimal" coefficients $\boldsymbol{\beta}$ that minimizes the error-term. 
What such an optimal solution evinces is dictated by the definition of our \textit{cost-function},  or \textit{loss-function}, which is simply metrics chosen to measure how much our predictions deviate from the "truth". 
A cost-function, $\text{Cost}(f,\mathcal{D} )$, is used to describe such a metric measuring a group of data-points, while a loss-function, $L(y, \hat{y})$, describes a metric regarding a single data-instance. 
The cost-function can be expressed in terms of the loss function
\[
\text{Cost}(f,\mathcal{D}) = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{y_i}).
\]
Through different choices of cost-function we end up with different methods for estimation and inference. 
Depending on priorities when choosing, different models provide things like easier interpretation, lower \textit{bias}, or lower \textit{variance}.

\julie{i forbindelse med valg av cost-funksjon kan jeg nevne bias-variance trade off så vidt eller typ alludere til det og evt si at vi kommer tilbake til dette senere - hva tenker dere? eller evt kan det komme her idk vi kan se på det}
\subsection{Bias-Variance Tradeoff}
\subsection{Ordinary least squares}
Ordinary least squares provides an unbiased estimation - meaning the expected distance between our prediction and the truth is zero. 
\julie{utdyp bias med ligninger osv, kanskje dette sist egt og cost funksjon først}
\subsection{Ridge Regression}
\subsection{Lasso Regression}