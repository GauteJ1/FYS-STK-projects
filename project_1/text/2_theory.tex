\mia{Hvor mye kilder trenger vi på dette?}

\subsection{Linear Regression}

Linear regression is one of the most common methods for modeling a relationship between an input dataset and an output dataset.
Given a dataset of $p$ input variables (commonly called predictors), $X=[x_1, \, x_2, \, \ldots, \, x_p]$ and a dataset of output variables (commonly called the response) we seek a linear model on the form
\begin{equation}
\hat{Y}=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j},
\end{equation}
to model the relationship, where the scalar $\hat{Y}$ is our prediction of the response. 
Here $\beta_0$ is the intercept, and each $\beta_j$ is the coefficient belonging to its corresponding predictor $x_j$. 
This equation is commonly written in vector form, 
\begin{equation}
\hat{Y}=\boldsymbol{X}^T\boldsymbol{\beta},
\end{equation}
or alternatively $n$ sets of data-points formatted in these parameters will be represented as a data-set on the form
\begin{equation}
\boldsymbol{\hat{Y}}=\boldsymbol{X}^T\boldsymbol{\beta},
\end{equation}
where $\boldsymbol{\hat{Y}}$ is the response-vector and $\boldsymbol{X}$ is a $n\times p$ -matrix with each row an input-vector of predictors.

\julie{$\uparrow$ Her er jeg kollosalt usikker på rad vs kolonne osv, pls check.}  \mia{Tror vi heller må utdype hva design matrix er. Sånn blir ikke direkte riktig å si hverken kolonne eller rad. X består av bare 1 i første kolonne, deretter ulike transformasjoner av inputen i de påfølgende kolonnene, for oss polynomer. Men inputen gir kolonnene, ikke radene, for å svare på det faktiske spørsmålet. Jeg kan skrive inn dette senere om ønskelig. }

As can be inferred by the equations above, linear regression bases itself on the assumption of a linear relationship between the predictors and the response.
The ``true" model is assumed the form 
\begin{equation}
Y=\hat{\beta_0}+\sum_{j=1}^{p}x_j\hat{\beta_j}+\epsilon,
\end{equation}
where $\epsilon$ is a \textit{error-term} or \textit{residual-term} representing all variance in the data not explainable by the linear model. 
Any variation due to randomness, noise or things like measurement-error is included in this term. \mia{Tror vi gjerne kan nevne allerede her at vi antar $\epsilon \sim  
\mathcal{N}(0,\sigma^2)$}


\subsection{Cost-Functions \& Loss-Functions}
The main objective when solving such a linear regression problem, is finding the ``optimal" coefficients $\boldsymbol{\beta}$ that minimizes the error-term. 
What such an optimal solution evinces is dictated by the definition of our \textit{cost-function},  or \textit{loss-function}, which is simply metrics chosen to measure how much our predictions deviate from the "truth". 
A cost-function, $\text{Cost}(f,\mathcal{D} )$, is used to describe such a metric measuring a group of data-points, while a loss-function, $L(y, \hat{y})$, describes a metric regarding a single data-instance. 
The cost-function can be expressed in terms of the loss function
\begin{equation}
\text{Cost}(f,\mathcal{D}) = \frac{1}{n}\sum_{i=1}^n L(y_i, \hat{y_i}).
\end{equation}
Through different choices of cost-function we end up with different methods for estimation and inference. 
Depending on priorities when choosing, different models provide things like easier interpretation, lower \textit{bias}, or lower \textit{variance}.


\julie{forskjellen på begrepene cost og loss er ikke diskutert i hastie, skal jeg prøve å grave frem en annen kilde? har evt denne; https://www.baeldung.com/cs/cost-vs-loss-vs-objective-function }

\subsection{Ordinary least squares}
Ordinary least squares provides an unbiased estimation - meaning the expected distance between our prediction and the truth is zero. 
\julie{utdyp bias med ligninger osv, kanskje dette sist egt og cost funksjon først}

\subsection{Ridge Regression}

\subsection{Lasso Regression}

\subsection{Mia}
train-test split and scaling

CV and bootstrap 

\subsection{Bias-Variance Tradeoff}

\mia{I will do this part, directly related to task e) pen and paper part}

\begin{equation}\label{biasvar}
    \fv{(\y - \yt)^2} = \text{Bias}(\yt)^2 + \text{var}(\yt) + \sigma^2
\end{equation}




