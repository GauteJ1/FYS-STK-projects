Neural networks is the primary type of models used in complex machine learning problems, as they are extremely agile and can perform well in a wide range of scenarios, including both regression and classification tasks.
They do however require large amounts of data to be trained and it is often hard to interpret how they make their predictions.
Classic models as linear and logistic regression are easier to interpret and simpler to fit to the data.
In this paper we compare the performance of linear regression, with both ordinary least squares and ridge, to neural networks with a range of architectures and activation functions, on the noisy Franke data set \cite{frank}, where we reuse our previous implementations of linear regression models \cite{project1}.
Furthermore we implement logistic regression, and compare its performance with neural networks on a classification task, based on the Wisconsin breast cancer data set \cite{breast_cancer_wisconsin}.

We find that a linear regression model performs equally well as our best neural network at predicting the Franke function data. 
%however we emphasize the fact that neural networks are computationally heavier and less interpretable than the linear regression models.
Both our best neural network model and the linear regression model achieves an $R^2$ value of $0.85$. Linear regression is preffered seeing as neural networks are computationally heavier and less interpretable.
We find that the accuracy of the neural network trained to solve a classification problem is $0.97$, equal to logistic regression. We prefer the logistic regression model as it has better recall.


%for this data set has two hidden layers with 24 nodes each, and ReLU as the activation function for both hidden layers. The batch size used is 1000 and the initial learning rate 0.01. The network is trained with a constant optimizer. to perform equally well as a logistic regression model. The best neural network is trained with RMSprop optimizer, two hidden layers of size 24 and 8, and sigmoid and leaky ReLU as the activation functions at the hidden layers. The accuracy score obtained from this network